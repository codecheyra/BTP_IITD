{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:57:43.494582Z",
     "iopub.status.busy": "2024-11-12T05:57:43.494195Z",
     "iopub.status.idle": "2024-11-12T05:59:05.158430Z",
     "shell.execute_reply": "2024-11-12T05:59:05.157131Z",
     "shell.execute_reply.started": "2024-11-12T05:57:43.494553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install /kaggle/input/nh-llama-2-7b/accelerate-0.21.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/nh-llama-2-7b/bitsandbytes-0.41.1-py3-none-any.whl\n",
    "!pip install /kaggle/input/nh-llama-2-7b/peft-0.4.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/nh-llama-2-7b/trl-0.5.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/nh-llama-2-7b/openapi_schema_pydantic-1.2.4-py3-none-any.whl\n",
    "!pip install /kaggle/input/nh-llama-2-7b/langsmith-0.0.22-py3-none-any.whl\n",
    "!pip install /kaggle/input/nh-llama-2-7b/langchain-0.0.264-py3-none-any.whl\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:59:05.160814Z",
     "iopub.status.busy": "2024-11-12T05:59:05.160465Z",
     "iopub.status.idle": "2024-11-12T05:59:05.168660Z",
     "shell.execute_reply": "2024-11-12T05:59:05.167814Z",
     "shell.execute_reply.started": "2024-11-12T05:59:05.160784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from string import Template\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# for training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "# for traing set\n",
    "from datasets import load_dataset\n",
    "from langchain.prompts import PromptTemplate\n",
    "import matplotlib.pyplot as plt\n",
    "import bitsandbytes as bnb\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:59:05.169871Z",
     "iopub.status.busy": "2024-11-12T05:59:05.169606Z",
     "iopub.status.idle": "2024-11-12T05:59:18.622139Z",
     "shell.execute_reply": "2024-11-12T05:59:18.621188Z",
     "shell.execute_reply.started": "2024-11-12T05:59:05.169849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# change model_name to the model of your choice.\n",
    "# This can be either name of the model on huggingface (requires internet) or path to the model\n",
    "model_name = \"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtyp=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# this should be set as False for finetuning\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:59:18.625351Z",
     "iopub.status.busy": "2024-11-12T05:59:18.625047Z",
     "iopub.status.idle": "2024-11-12T05:59:19.349865Z",
     "shell.execute_reply": "2024-11-12T05:59:19.348898Z",
     "shell.execute_reply.started": "2024-11-12T05:59:18.625319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # load training data\n",
    "train_dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/kaggle-llm-science-exam/train.csv\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:59:19.351312Z",
     "iopub.status.busy": "2024-11-12T05:59:19.351031Z",
     "iopub.status.idle": "2024-11-12T05:59:19.357296Z",
     "shell.execute_reply": "2024-11-12T05:59:19.356264Z",
     "shell.execute_reply.started": "2024-11-12T05:59:19.351287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# prepare template \n",
    "template = \"\"\"Answer the following multiple choice question by giving the most appropriate response. Answer should be one among [A, B, C, D, E]\n",
    "\n",
    "Question: {prompt}\\n\n",
    "A) {a}\\n\n",
    "B) {b}\\n\n",
    "C) {c}\\n\n",
    "D) {d}\\n\n",
    "E) {e}\\n\n",
    "\n",
    "### Answer: {answer}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['prompt', 'a', 'b', 'c', 'd', 'e', 'answer'])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:59:19.358784Z",
     "iopub.status.busy": "2024-11-12T05:59:19.358495Z",
     "iopub.status.idle": "2024-11-12T05:59:19.371494Z",
     "shell.execute_reply": "2024-11-12T05:59:19.370506Z",
     "shell.execute_reply.started": "2024-11-12T05:59:19.358758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# display sample to see template\n",
    "sample = train_dataset['train'][0]\n",
    "display(Markdown(prompt.format(prompt=sample['prompt'], \n",
    "                               a=sample['A'], \n",
    "                               b=sample['B'], \n",
    "                               c=sample['C'], \n",
    "                               d=sample['D'], \n",
    "                               e=sample['E'], \n",
    "                               answer=sample['answer'])))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:59:19.373334Z",
     "iopub.status.busy": "2024-11-12T05:59:19.372970Z",
     "iopub.status.idle": "2024-11-12T05:59:19.381471Z",
     "shell.execute_reply": "2024-11-12T05:59:19.380525Z",
     "shell.execute_reply.started": "2024-11-12T05:59:19.373289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_text(example):\n",
    "    \"\"\" fill inputs in promt for a sample  \"\"\"\n",
    "    text = prompt.format(prompt=example['prompt'], \n",
    "                         a=example['A'], \n",
    "                         b=example['B'], \n",
    "                         c=example['C'], \n",
    "                         d=example['D'], \n",
    "                         e=example['E'], \n",
    "                         answer=example['answer'])\n",
    "    return {\"text\": text}\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:59:19.382995Z",
     "iopub.status.busy": "2024-11-12T05:59:19.382736Z",
     "iopub.status.idle": "2024-11-12T05:59:19.506228Z",
     "shell.execute_reply": "2024-11-12T05:59:19.505321Z",
     "shell.execute_reply.started": "2024-11-12T05:59:19.382972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(format_text)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:59:19.507971Z",
     "iopub.status.busy": "2024-11-12T05:59:19.507605Z",
     "iopub.status.idle": "2024-11-12T05:59:19.512759Z",
     "shell.execute_reply": "2024-11-12T05:59:19.511845Z",
     "shell.execute_reply.started": "2024-11-12T05:59:19.507940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# check model structure\n",
    "model\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:59:19.516687Z",
     "iopub.status.busy": "2024-11-12T05:59:19.516364Z",
     "iopub.status.idle": "2024-11-12T05:59:19.528314Z",
     "shell.execute_reply": "2024-11-12T05:59:19.527507Z",
     "shell.execute_reply.started": "2024-11-12T05:59:19.516658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_linear_layers(model):\n",
    "    \"\"\" find linear layers in given transformer model \"\"\"\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        # 4 bits for qlora\n",
    "        if isinstance(module, bnb.nn.Linear4bit): \n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    print(f\"LoRA module names: {list(lora_module_names)}\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "target_modules = find_linear_layers(model)\n",
    "#for llama 2 (they need different target module)\n",
    "qlora_config = LoraConfig(\n",
    "    r=16,  # dimension of the updated matrices\n",
    "    lora_alpha=64,  # parameter for scaling\n",
    "    target_modules=target_modules, # this chooses on which layers QLoRA is applied\n",
    "    lora_dropout=0.1,  # dropout probability for layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:59:19.529565Z",
     "iopub.status.busy": "2024-11-12T05:59:19.529320Z",
     "iopub.status.idle": "2024-11-12T05:59:19.542745Z",
     "shell.execute_reply": "2024-11-12T05:59:19.541814Z",
     "shell.execute_reply.started": "2024-11-12T05:59:19.529544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# \"max_steps=1\" is just for testing execution\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./SFT-llama2-7b\", \n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=20,\n",
    "    logging_strategy=\"steps\",\n",
    "    warmup_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    max_steps=1,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    run_name=\"baseline-llama2-sft\",\n",
    "    save_total_limit=1,  # can be increased, but but beware of kaggle notebook output size limit\n",
    "    report_to=\"none\"\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:59:19.544117Z",
     "iopub.status.busy": "2024-11-12T05:59:19.543798Z",
     "iopub.status.idle": "2024-11-12T06:00:15.613122Z",
     "shell.execute_reply": "2024-11-12T06:00:15.612172Z",
     "shell.execute_reply.started": "2024-11-12T05:59:19.544087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "supervised_finetuning_trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=qlora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=3000,\n",
    "    data_collator=DataCollatorForCompletionOnlyLM(tokenizer=tokenizer, \n",
    "                                                  response_template=\"Answer:\")\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T06:00:15.614547Z",
     "iopub.status.busy": "2024-11-12T06:00:15.614249Z",
     "iopub.status.idle": "2024-11-12T06:00:37.956655Z",
     "shell.execute_reply": "2024-11-12T06:00:37.955657Z",
     "shell.execute_reply.started": "2024-11-12T06:00:15.614522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "supervised_finetuning_trainer.train()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T06:00:37.958721Z",
     "iopub.status.busy": "2024-11-12T06:00:37.958340Z",
     "iopub.status.idle": "2024-11-12T06:00:38.248091Z",
     "shell.execute_reply": "2024-11-12T06:00:38.247032Z",
     "shell.execute_reply.started": "2024-11-12T06:00:37.958688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_to_save = supervised_finetuning_trainer.model.module if hasattr(supervised_finetuning_trainer.model, 'module') else supervised_finetuning_trainer.model\n",
    "model_to_save.save_pretrained(\"outputs\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T06:00:38.249790Z",
     "iopub.status.busy": "2024-11-12T06:00:38.249420Z",
     "iopub.status.idle": "2024-11-12T06:00:38.821834Z",
     "shell.execute_reply": "2024-11-12T06:00:38.820898Z",
     "shell.execute_reply.started": "2024-11-12T06:00:38.249759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained('outputs')\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T06:00:38.823351Z",
     "iopub.status.busy": "2024-11-12T06:00:38.823055Z",
     "iopub.status.idle": "2024-11-12T06:00:38.829403Z",
     "shell.execute_reply": "2024-11-12T06:00:38.828382Z",
     "shell.execute_reply.started": "2024-11-12T06:00:38.823324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# same prompt as before\n",
    "template = \"\"\"Answer the following multiple choice question by giving the most appropriate response. Answer should be one among [A, B, C, D, E]\n",
    "\n",
    "Question: {prompt}\\n\n",
    "A) {a}\\n\n",
    "B) {b}\\n\n",
    "C) {c}\\n\n",
    "D) {d}\\n\n",
    "E) {e}\\n\n",
    "\n",
    "### Answer: {answer}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['prompt', 'a', 'b', 'c', 'd', 'e', 'answer'])\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T06:00:38.830721Z",
     "iopub.status.busy": "2024-11-12T06:00:38.830412Z",
     "iopub.status.idle": "2024-11-12T06:00:39.639251Z",
     "shell.execute_reply": "2024-11-12T06:00:39.638392Z",
     "shell.execute_reply.started": "2024-11-12T06:00:38.830697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# We don't have answers for test\n",
    "def format_text_test(example):\n",
    "    text = prompt.format(prompt=example['prompt'], \n",
    "                         a=example['A'], \n",
    "                         b=example['B'], \n",
    "                         c=example['C'], \n",
    "                         d=example['D'], \n",
    "                         e=example['E'], \n",
    "                         answer='')\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "test_dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n",
    "test_dataset = test_dataset.map(format_text_test)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T06:00:39.640609Z",
     "iopub.status.busy": "2024-11-12T06:00:39.640318Z",
     "iopub.status.idle": "2024-11-12T06:00:39.648968Z",
     "shell.execute_reply": "2024-11-12T06:00:39.648124Z",
     "shell.execute_reply.started": "2024-11-12T06:00:39.640584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Perplexity(nn.Module):\n",
    "    def __init__(self, reduce: bool = True):\n",
    "        super().__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        perplexity = []\n",
    "        for i in range(labels.shape[0]):\n",
    "            perplexity.append(self.loss_fn(shift_logits[i], shift_labels[i]))\n",
    "        perplexity = torch.stack(perplexity, dim=0)\n",
    "        if self.reduce:\n",
    "            perplexity = torch.mean(perplexity)\n",
    "        return perplexity \n",
    "    \n",
    "perp = Perplexity()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T06:00:39.650241Z",
     "iopub.status.busy": "2024-11-12T06:00:39.649993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for idx in tqdm(range(len(test_dataset[\"train\"])), total=len(test_dataset[\"train\"])):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        cols = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "        perps = []\n",
    "        samples = []\n",
    "        for col in cols:\n",
    "            prompt = test_dataset['train'][idx]['text']\n",
    "            samples.append(prompt + col)\n",
    "        inputs = tokenizer(samples, return_tensors=\"pt\", add_special_tokens=False, padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "        output = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        output = output.logits\n",
    "        labels = inputs[\"input_ids\"]\n",
    "        labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100)\n",
    "        for j in range(len(cols)):\n",
    "            p = perp(output[j].unsqueeze(0), labels[j].unsqueeze(0))\n",
    "            perps.append(p.detach().cpu())\n",
    "            \n",
    "        del inputs\n",
    "        del labels\n",
    "        del output\n",
    "        del p\n",
    "\n",
    "    perps = np.array(perps)\n",
    "    predictions = [np.array(cols)[np.argsort(perps)]]\n",
    "    preds.append(predictions)\n",
    "    print(\"done\")\n",
    "print(\"total done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T13:39:01.937617Z",
     "iopub.status.busy": "2023-08-19T13:39:01.937215Z",
     "iopub.status.idle": "2023-08-19T13:39:01.944443Z",
     "shell.execute_reply": "2023-08-19T13:39:01.942939Z",
     "shell.execute_reply.started": "2023-08-19T13:39:01.937586Z"
    }
   },
   "source": [
    "### format predictions to sumbission format and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_prediction(row, k=1):\n",
    "    best_k_preds = row[0][:k]\n",
    "    return ' '.join(best_k_preds)\n",
    "\n",
    "test_df = pd.DataFrame(preds)\n",
    "format_prediction(test_df.iloc[0, :])\n",
    "test_df['prediction'] = test_df.apply(lambda x: format_prediction(x), axis=1)\n",
    "test_df['id'] = test_df.index\n",
    "\n",
    "submission = test_df[['id', 'prediction']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# After fine-tuning, save the model to a directory in the /kaggle/working path\n",
    "model_to_save = supervised_finetuning_trainer.model.module if hasattr(supervised_finetuning_trainer.model, 'module') else supervised_finetuning_trainer.model\n",
    "model_path = \"/kaggle/working/fine_tuned_model\"\n",
    "model_to_save.save_pretrained(model_path)\n",
    "print(\"Model saved to /kaggle/working/fine_tuned_model\")\n",
    "\n",
    "# Zip the model directory for easy download\n",
    "!zip -r /kaggle/working/fine_tuned_model.zip {model_path}\n",
    "print(\"Model zipped for download\")\n",
    "print(\"done\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6169864,
     "sourceId": 54662,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 10114681,
     "sourceId": 88362,
     "sourceType": "competition"
    },
    {
     "datasetId": 3601853,
     "sourceId": 6266221,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3616549,
     "sourceId": 6303489,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30528,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
