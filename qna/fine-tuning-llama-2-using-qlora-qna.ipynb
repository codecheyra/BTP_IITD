{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:09:44.513273Z",
     "iopub.status.busy": "2024-11-10T17:09:44.512600Z",
     "iopub.status.idle": "2024-11-10T17:11:14.395852Z",
     "shell.execute_reply": "2024-11-10T17:11:14.394649Z",
     "shell.execute_reply.started": "2024-11-10T17:09:44.513234Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install /kaggle/input/nh-llama-2-7b/accelerate-0.21.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/nh-llama-2-7b/bitsandbytes-0.41.1-py3-none-any.whl\n",
    "!pip install /kaggle/input/nh-llama-2-7b/peft-0.4.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/nh-llama-2-7b/trl-0.5.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/nh-llama-2-7b/openapi_schema_pydantic-1.2.4-py3-none-any.whl\n",
    "!pip install /kaggle/input/nh-llama-2-7b/langsmith-0.0.22-py3-none-any.whl\n",
    "!pip install /kaggle/input/nh-llama-2-7b/langchain-0.0.264-py3-none-any.whl\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:11:14.398584Z",
     "iopub.status.busy": "2024-11-10T17:11:14.398181Z",
     "iopub.status.idle": "2024-11-10T17:11:30.551764Z",
     "shell.execute_reply": "2024-11-10T17:11:30.550761Z",
     "shell.execute_reply.started": "2024-11-10T17:11:14.398544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from string import Template\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# for training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "# for traing set\n",
    "from datasets import load_dataset\n",
    "from langchain.prompts import PromptTemplate\n",
    "import matplotlib.pyplot as plt\n",
    "import bitsandbytes as bnb\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:11:30.553636Z",
     "iopub.status.busy": "2024-11-10T17:11:30.552894Z",
     "iopub.status.idle": "2024-11-10T17:13:07.443613Z",
     "shell.execute_reply": "2024-11-10T17:13:07.442752Z",
     "shell.execute_reply.started": "2024-11-10T17:11:30.553605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# change model_name to the model of your choice.\n",
    "# This can be either name of the model on huggingface (requires internet) or path to the model\n",
    "model_name = \"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtyp=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# this should be set as False for finetuning\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before finetuning answers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/finetuningllmqna/test.csv\")\n",
    "\n",
    "template = \"\"\"Provide a detailed answer to the following question.\n",
    "Question: {Question}\n",
    "### Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['Question'])\n",
    "\n",
    "def format_text_test(example):\n",
    "    text = prompt.format(Question=example['Question'])\n",
    "    return {\"text\": text}\n",
    "\n",
    "test_dataset = test_dataset.map(format_text_test)\n",
    "\n",
    "preds_before_finetuning = []\n",
    "for idx in tqdm(range(len(test_dataset[\"train\"])), total=len(test_dataset[\"train\"])):\n",
    "    prompt = test_dataset['train'][idx]['text']\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    inputs = {key: value for key, value in inputs.items() if key != \"token_type_ids\"}\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    Answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    preds_before_finetuning.append(Answer.split(\"### Answer:\")[-1].strip())\n",
    "os.makedirs(\"/kaggle/working/test_responses_before_finetuning\", exist_ok=True)\n",
    "test_df_before = pd.DataFrame({\n",
    "    \"Question\": [test_dataset[\"train\"][i][\"Question\"] for i in range(len(test_dataset[\"train\"]))],\n",
    "    \"Answer\": preds_before_finetuning\n",
    "})\n",
    "test_df_before.to_csv(\"/kaggle/working/test_responses_before_finetuning/responses_before_finetuning.csv\", index=False)\n",
    "print(\"Responses before fine-tuning saved to /kaggle/working/test_responses_before_finetuning/responses_before_finetuning.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:13:07.447164Z",
     "iopub.status.busy": "2024-11-10T17:13:07.446570Z",
     "iopub.status.idle": "2024-11-10T17:13:07.644896Z",
     "shell.execute_reply": "2024-11-10T17:13:07.644004Z",
     "shell.execute_reply.started": "2024-11-10T17:13:07.447134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # load training data\n",
    "# train_dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/kaggle-llm-science-exam/train.csv\")\n",
    "# print(\"done\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/finetuningllmqna/train.csv\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:13:07.646454Z",
     "iopub.status.busy": "2024-11-10T17:13:07.646162Z",
     "iopub.status.idle": "2024-11-10T17:13:07.652928Z",
     "shell.execute_reply": "2024-11-10T17:13:07.651875Z",
     "shell.execute_reply.started": "2024-11-10T17:13:07.646428Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define template for question-answer pair\n",
    "template = \"\"\"Provide a detailed answer to the following question.\n",
    "\n",
    "Question: {Question}\n",
    "\n",
    "### Answer: {Answer}\"\"\"\n",
    "\n",
    "# Prepare prompt for fine-tuning\n",
    "prompt = PromptTemplate(template=template, input_variables=['Question', 'Answer'])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:13:07.654559Z",
     "iopub.status.busy": "2024-11-10T17:13:07.654205Z",
     "iopub.status.idle": "2024-11-10T17:13:07.667649Z",
     "shell.execute_reply": "2024-11-10T17:13:07.666571Z",
     "shell.execute_reply.started": "2024-11-10T17:13:07.654526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# display sample to see template\n",
    "sample = train_dataset['train'][0]\n",
    "display(Markdown(prompt.format(Question=sample['Question'], Answer=sample['Answer'])))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:13:07.669262Z",
     "iopub.status.busy": "2024-11-10T17:13:07.668897Z",
     "iopub.status.idle": "2024-11-10T17:13:07.676293Z",
     "shell.execute_reply": "2024-11-10T17:13:07.675348Z",
     "shell.execute_reply.started": "2024-11-10T17:13:07.669224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# function for the question-answer dataset\n",
    "def format_text(example):\n",
    "    \"\"\"Fill inputs in prompt for each sample\"\"\"\n",
    "    text = prompt.format(Question=example['Question'], Answer=example['Answer'])\n",
    "    return {\"text\": text}\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:13:07.677770Z",
     "iopub.status.busy": "2024-11-10T17:13:07.677446Z",
     "iopub.status.idle": "2024-11-10T17:13:07.739132Z",
     "shell.execute_reply": "2024-11-10T17:13:07.738321Z",
     "shell.execute_reply.started": "2024-11-10T17:13:07.677746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(format_text)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:13:07.741034Z",
     "iopub.status.busy": "2024-11-10T17:13:07.740381Z",
     "iopub.status.idle": "2024-11-10T17:13:07.745348Z",
     "shell.execute_reply": "2024-11-10T17:13:07.744456Z",
     "shell.execute_reply.started": "2024-11-10T17:13:07.741001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# check model structure\n",
    "model\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:13:07.749299Z",
     "iopub.status.busy": "2024-11-10T17:13:07.748890Z",
     "iopub.status.idle": "2024-11-10T17:13:07.759856Z",
     "shell.execute_reply": "2024-11-10T17:13:07.758973Z",
     "shell.execute_reply.started": "2024-11-10T17:13:07.749267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define target modules for QLoRA\n",
    "def find_linear_layers(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit): \n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "target_modules = find_linear_layers(model)\n",
    "qlora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:13:07.761186Z",
     "iopub.status.busy": "2024-11-10T17:13:07.760888Z",
     "iopub.status.idle": "2024-11-10T17:13:07.775965Z",
     "shell.execute_reply": "2024-11-10T17:13:07.775128Z",
     "shell.execute_reply.started": "2024-11-10T17:13:07.761155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./SFT-llama2-7b\", \n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=20,\n",
    "    logging_strategy=\"steps\",\n",
    "    warmup_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    max_steps=1,  # Adjust this based on your resources\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    run_name=\"baseline-llama2-sft\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:13:07.777697Z",
     "iopub.status.busy": "2024-11-10T17:13:07.777408Z",
     "iopub.status.idle": "2024-11-10T17:14:06.151312Z",
     "shell.execute_reply": "2024-11-10T17:14:06.150359Z",
     "shell.execute_reply.started": "2024-11-10T17:13:07.777674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize trainer for fine-tuning\n",
    "supervised_finetuning_trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=qlora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=3000,\n",
    "    data_collator=DataCollatorForCompletionOnlyLM(tokenizer=tokenizer, response_template=\"Answer:\")\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:14:06.152965Z",
     "iopub.status.busy": "2024-11-10T17:14:06.152604Z",
     "iopub.status.idle": "2024-11-10T17:14:16.561005Z",
     "shell.execute_reply": "2024-11-10T17:14:16.560120Z",
     "shell.execute_reply.started": "2024-11-10T17:14:06.152931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "supervised_finetuning_trainer.train()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:14:16.562772Z",
     "iopub.status.busy": "2024-11-10T17:14:16.562391Z",
     "iopub.status.idle": "2024-11-10T17:14:16.859444Z",
     "shell.execute_reply": "2024-11-10T17:14:16.858243Z",
     "shell.execute_reply.started": "2024-11-10T17:14:16.562736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_to_save = supervised_finetuning_trainer.model.module if hasattr(supervised_finetuning_trainer.model, 'module') else supervised_finetuning_trainer.model\n",
    "model_to_save.save_pretrained(\"outputs\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:14:16.861039Z",
     "iopub.status.busy": "2024-11-10T17:14:16.860727Z",
     "iopub.status.idle": "2024-11-10T17:14:17.451938Z",
     "shell.execute_reply": "2024-11-10T17:14:17.450942Z",
     "shell.execute_reply.started": "2024-11-10T17:14:16.861010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained('outputs')\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:14:17.453261Z",
     "iopub.status.busy": "2024-11-10T17:14:17.452969Z",
     "iopub.status.idle": "2024-11-10T17:14:17.459747Z",
     "shell.execute_reply": "2024-11-10T17:14:17.458688Z",
     "shell.execute_reply.started": "2024-11-10T17:14:17.453235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "# test_dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n",
    "# Template without answer for inference\n",
    "template = \"\"\"Provide a detailed answer to the following question.\n",
    "\n",
    "Question: {Question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['Question'])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:14:17.461160Z",
     "iopub.status.busy": "2024-11-10T17:14:17.460835Z",
     "iopub.status.idle": "2024-11-10T17:14:17.658954Z",
     "shell.execute_reply": "2024-11-10T17:14:17.658111Z",
     "shell.execute_reply.started": "2024-11-10T17:14:17.461132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Modify format function for test set\n",
    "def format_text_test(example):\n",
    "    text = prompt.format(Question=example['Question'])\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Prepare test data\n",
    "test_dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/finetuningllmqna/test.csv\")\n",
    "test_dataset = test_dataset.map(format_text_test)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:14:17.660231Z",
     "iopub.status.busy": "2024-11-10T17:14:17.659938Z",
     "iopub.status.idle": "2024-11-10T17:14:17.668790Z",
     "shell.execute_reply": "2024-11-10T17:14:17.667740Z",
     "shell.execute_reply.started": "2024-11-10T17:14:17.660205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Perplexity(nn.Module):\n",
    "    def __init__(self, reduce: bool = True):\n",
    "        super().__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        perplexity = []\n",
    "        for i in range(labels.shape[0]):\n",
    "            perplexity.append(self.loss_fn(shift_logits[i], shift_labels[i]))\n",
    "        perplexity = torch.stack(perplexity, dim=0)\n",
    "        if self.reduce:\n",
    "            perplexity = torch.mean(perplexity)\n",
    "        return perplexity \n",
    "    \n",
    "perp = Perplexity()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:14:17.670346Z",
     "iopub.status.busy": "2024-11-10T17:14:17.670006Z",
     "iopub.status.idle": "2024-11-10T17:15:00.560321Z",
     "shell.execute_reply": "2024-11-10T17:15:00.559223Z",
     "shell.execute_reply.started": "2024-11-10T17:14:17.670313Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate answers for test questions\n",
    "preds = []\n",
    "for idx in tqdm(range(len(test_dataset[\"train\"])), total=len(test_dataset[\"train\"])):\n",
    "    prompt = test_dataset['train'][idx]['text']\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Exclude `token_type_ids` to avoid passing it to `generate`\n",
    "    inputs = {key: value for key, value in inputs.items() if key != \"token_type_ids\"}\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    Answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    preds.append(Answer.split(\"### Answer:\")[-1].strip())  # Extract only the answer part\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T13:39:01.937617Z",
     "iopub.status.busy": "2023-08-19T13:39:01.937215Z",
     "iopub.status.idle": "2023-08-19T13:39:01.944443Z",
     "shell.execute_reply": "2023-08-19T13:39:01.942939Z",
     "shell.execute_reply.started": "2023-08-19T13:39:01.937586Z"
    }
   },
   "source": [
    "### format predictions to sumbission format and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:15:00.561867Z",
     "iopub.status.busy": "2024-11-10T17:15:00.561565Z",
     "iopub.status.idle": "2024-11-10T17:15:00.572887Z",
     "shell.execute_reply": "2024-11-10T17:15:00.572067Z",
     "shell.execute_reply.started": "2024-11-10T17:15:00.561839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare submission file\n",
    "import os\n",
    "os.makedirs(\"kaggle/finetuningllmqna\", exist_ok=True)\n",
    "test_df = pd.DataFrame({\n",
    "    \"Question\": [test_dataset[\"train\"][i][\"Question\"] for i in range(len(test_dataset[\"train\"]))],\n",
    "    \"Answer\": preds\n",
    "})\n",
    "test_df.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
    "print(\"Submission file created successfully.\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:15:00.574364Z",
     "iopub.status.busy": "2024-11-10T17:15:00.574029Z",
     "iopub.status.idle": "2024-11-10T17:15:07.015660Z",
     "shell.execute_reply": "2024-11-10T17:15:07.014557Z",
     "shell.execute_reply.started": "2024-11-10T17:15:00.574339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# After fine-tuning, save the model to a directory in the /kaggle/working path\n",
    "model_to_save = supervised_finetuning_trainer.model.module if hasattr(supervised_finetuning_trainer.model, 'module') else supervised_finetuning_trainer.model\n",
    "model_path = \"/kaggle/working/fine_tuned_model\"\n",
    "model_to_save.save_pretrained(model_path)\n",
    "print(\"Model saved to /kaggle/working/fine_tuned_model\")\n",
    "\n",
    "# Zip the model directory for easy download\n",
    "!zip -r /kaggle/working/fine_tuned_model.zip {model_path}\n",
    "print(\"Model zipped for download\")\n",
    "print(\"done\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6169864,
     "sourceId": 54662,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 10114681,
     "sourceId": 88362,
     "sourceType": "competition"
    },
    {
     "datasetId": 3601853,
     "sourceId": 6266221,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3616549,
     "sourceId": 6303489,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30528,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
